{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwgNuXkcMMVt",
        "outputId": "8974ea5f-c6b5-4a72-97c9-d8c2322b78d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=2914ac6e1bb1a9b8a2247b98098a1e6913f6a51598c5871464da455f6d8191e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkFiles\n",
        "import time\n",
        "from pyspark.sql.functions import year, round\n",
        "import time"
      ],
      "metadata": {
        "id": "9VoyYmjCTNvW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
        "\n",
        "\n",
        "# Read in the AWS S3 bucket into a DataFrame\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(\"file://\" + SparkFiles.get(\"home_sales_revised.csv\"), header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "5UHupM9RTQKk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract 'year' from the 'date' column\n",
        "df = df.withColumn(\"year\", year(\"date\"))"
      ],
      "metadata": {
        "id": "1-ayWRq9TSVJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary table called home_sales\n",
        "df.createOrReplaceTempView(\"home_sales\")"
      ],
      "metadata": {
        "id": "iKBYn6knTSSj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir as colunas do DataFrame\n",
        "df_columns = df.columns\n",
        "print(df_columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-m9SdYVzHV4",
        "outputId": "ce6a56e2-896d-410e-d88b-f6d6584a38dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'date', 'date_built', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'year']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Query for average price of a four-bedroom house sold in each year\n",
        "query3 = \"SELECT year, ROUND(AVG(price), 2) as avg_price FROM home_sales WHERE bedrooms = 4 GROUP BY year\"\n",
        "result3 = spark.sql(query3)\n",
        "result3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWlruexXTSKS",
        "outputId": "eb8bb851-d81e-4561-a5b3-c878667cc5e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|year|avg_price|\n",
            "+----+---------+\n",
            "|2022|296363.88|\n",
            "|2019| 300263.7|\n",
            "|2020|298353.78|\n",
            "|2021|301819.44|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Query for average price of a home with three bedrooms and three bathrooms\n",
        "query4 = \"SELECT year, ROUND(AVG(price), 2) as avg_price FROM home_sales WHERE bedrooms = 3 AND bathrooms = 3 GROUP BY year\"\n",
        "result4 = spark.sql(query4)\n",
        "result4.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3svcDtTTZSo",
        "outputId": "7073d2a9-2fe3-43df-9ad7-1908729e1dfe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|year|avg_price|\n",
            "+----+---------+\n",
            "|2022|292725.69|\n",
            "|2019|287287.82|\n",
            "|2020|294204.16|\n",
            "|2021|294211.46|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Query for average price of a home with specific features for each year\n",
        "query5 = \"SELECT year, ROUND(AVG(price), 2) as avg_price FROM home_sales WHERE bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000 GROUP BY year\"\n",
        "result5 = spark.sql(query5)\n",
        "result5.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8WYPSYZTcdy",
        "outputId": "cd6ef818-333e-4655-fd51-098fea41c273"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|year|avg_price|\n",
            "+----+---------+\n",
            "|2022|290242.99|\n",
            "|2019|289859.14|\n",
            "|2020|292289.09|\n",
            "|2021|296330.96|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Query for view rating for homes costing more than or equal to $350,000 with runtime\n",
        "start_time = time.time()\n",
        "query6 = \"SELECT view, ROUND(AVG(price), 2) as avg_price FROM home_sales WHERE price >= 350000 GROUP BY view\"\n",
        "result6 = spark.sql(query6)\n",
        "end_time = time.time()\n",
        "runtime_query6 = \"{:.2f}\".format(end_time - start_time)\n",
        "result6.show()\n",
        "print(f\"Runtime for query 6: {runtime_query6} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guI-dXFFic_0",
        "outputId": "7caf4bd7-55fa-41bf-843c-2402b13aa54e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|view| avg_price|\n",
            "+----+----------+\n",
            "|  31| 399856.95|\n",
            "|  85|1056336.74|\n",
            "|  65| 736679.93|\n",
            "|  53|  755214.8|\n",
            "|  78|1080649.37|\n",
            "|  34| 401419.75|\n",
            "|  81|1053472.79|\n",
            "|  28| 402124.62|\n",
            "|  76|1058802.78|\n",
            "|  26| 401506.97|\n",
            "|  27| 399537.66|\n",
            "|  44| 400598.05|\n",
            "|  12| 401501.32|\n",
            "|  91|1137372.73|\n",
            "|  22| 402022.68|\n",
            "|  93|1026006.06|\n",
            "|  47|  398447.5|\n",
            "|   1| 401044.25|\n",
            "|  52| 733780.26|\n",
            "|  13| 398917.98|\n",
            "+----+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Runtime for query 6: 0.07 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Cache the temporary table home_sales\n",
        "spark.catalog.cacheTable(\"home_sales\")"
      ],
      "metadata": {
        "id": "yWu2ranAiMw0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Run the cached query and measure runtime\n",
        "start_time_cached = time.time()\n",
        "result_cached = spark.sql(query6)\n",
        "end_time_cached = time.time()\n",
        "runtime_cached = \"{:.2f}\".format(end_time_cached - start_time_cached)\n",
        "print(f\"Runtime for cached query: {runtime_cached} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0stagUNi5UM",
        "outputId": "5b91dbad-2c3f-4a16-9a3f-30f3f08eb621"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime for cached query: 0.03 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Partition by the \"date_built\" field on the formatted parquet home sales data\n",
        "df.write.partitionBy(\"date_built\").parquet(\"path/to/formatted_parquet_data\")"
      ],
      "metadata": {
        "id": "zvnYEn6PiP2m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Create a temporary table for the parquet data\n",
        "parquet_df = spark.read.parquet(\"path/to/formatted_parquet_data\")\n",
        "parquet_df.createOrReplaceTempView(\"parquet_data\")\n"
      ],
      "metadata": {
        "id": "YwQz2473i_q9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Run the query on the parquet temporary table and measure runtime\n",
        "start_time_parquet = time.time()\n",
        "result_parquet = spark.sql(query6)\n",
        "end_time_parquet = time.time()\n",
        "runtime_parquet = end_time_parquet - start_time_parquet\n",
        "runtime_parquet_formatted = \"{:.2f}\".format(runtime_parquet)\n",
        "print(f\"Runtime for parquet query: {runtime_parquet_formatted} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAQiowyI0GmJ",
        "outputId": "b3ad179a-6dce-4d97-a542-342219653f42"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime for parquet query: 0.07 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Uncache the home_sales temporary table and verify\n",
        "spark.catalog.uncacheTable(\"home_sales\")\n",
        "if spark.catalog.isCached(\"home_sales\"):\n",
        "    print(\"home_sales table is still cached.\")\n",
        "else:\n",
        "    print(\"home_sales table is uncached.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoRZTZZL0f2W",
        "outputId": "9ed3925a-50fb-4201-a2ea-cfb23fdd9f5f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "home_sales table is uncached.\n"
          ]
        }
      ]
    }
  ]
}